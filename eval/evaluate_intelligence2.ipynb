{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "from pydantic import BaseModel\n",
    "import openai\n",
    "from typing import List, Dict, Literal\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "dotenv.load_dotenv(override=True)\n",
    "\n",
    "API_URL = os.getenv('API_URL')\n",
    "API_KEY = os.getenv('API_KEY')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_chat_message(messages: List[Dict[str, str]]) -> dict:\n",
    "    \"\"\"\n",
    "    Send a chat message to the API with up to 3 retries on failure\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dictionaries, each containing 'sender' and 'message'\n",
    "        base_url: Base URL of the API server\n",
    "    \n",
    "    Returns:\n",
    "        dict: Response from the API\n",
    "    \"\"\"\n",
    "    endpoint = API_URL\n",
    "    chai_request = {\n",
    "        \"memory\": \"The year is 500 AD. You are the great sage of Babylon. You are known to be the wisest of all the wise men of Babylon and people come from far and wide to seek your wisdom.\",\n",
    "        \"prompt\": \"An engaging conversation with Great Sage of Babylon.\",\n",
    "        \"bot_name\": \"Great Sage of Babylon\",\n",
    "        \"user_name\": \"User\",\n",
    "        \"chat_history\": messages,\n",
    "    }\n",
    "    \n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.post(endpoint, json=chai_request, headers={\"Authorization\": f\"Bearer {API_KEY}\"})\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt == max_retries - 1:  # Last attempt]\n",
    "                print(f\"Failed to get response after {max_retries} attempts\")\n",
    "                raise  # Re-raise the last exception\n",
    "            time.sleep(1 * (attempt + 1))  # Exponential backoff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "\n",
    "class TestCase(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "class TestCaseResponse(BaseModel):\n",
    "    test_cases: List[TestCase]\n",
    "\n",
    "\n",
    "def generate_test_cases(num_cases: int = 10) -> List[TestCase]:\n",
    "    \"\"\"\n",
    "    Use GPT-4 to generate diverse test cases\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"\n",
    "    Generate challenging test cases to evaluate an AI model that acts as a Babylonian sage from 500 AD.\n",
    "    We are specifically interested in testing the model's knowledge.\n",
    "\n",
    "    Keep in mind that the model we are testing is optimized for social interactions. It is uniquely good at being engaging, entertaining, and \n",
    "    aligned with the user who is interacting with it. It is not optimized for logical reasoning, so take that into account when generating test cases.\n",
    "\n",
    "    Furthermore (and this is very important), I want to evaluate the model objectively. So try to generate test cases that have objective answers. \n",
    "    However, the answers should be somewhat obscure, so that a small LLM may not be able to answer it. For example, the question \"What is the name of \n",
    "    the Babylonian king who is famous for creating one of the earliest and most complete written legal codes?\" has an objective answer (Hammurabi), \n",
    "    but it is not a well-known fact taught in high school history classes, so it's probably not obscure enough to be a good test case.\n",
    "\n",
    "    For example, the question \"What is the capital of France?\" has an objective answer (Paris). The question \"Who are Barack and Michelle Obama's children?\",\n",
    "    it has an objective answer (Malia and Sasha). The question \"What are the best poems from ancient Babylon\" is not objective, because there are many different\n",
    "    opinions on what the best poems from ancient Babylon are. Where possible, generate test cases that have objective answers, and that relate to questions\n",
    "    you can imagine a traveler in the ancient world might ask of a Babylonian sage.\n",
    "\n",
    "\n",
    "    For each question, provide:\n",
    "    \n",
    "    1. The question text\n",
    "    2. Correct answer\n",
    "\n",
    "    Format as JSON array of objects with fields: question, expected\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Generate {num_cases} diverse test cases.\"}\n",
    "            ],\n",
    "            temperature=0.4,\n",
    "            response_format=TestCaseResponse\n",
    "        )\n",
    "        message = response.choices[0].message\n",
    "        json_data = json.loads(message.content)\n",
    "        # Convert dictionary objects to TestCase objects\n",
    "        return [TestCase(**test_case) for test_case in json_data['test_cases']]\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating test cases: {e}\")\n",
    "        return []\n",
    "\n",
    "# Update the last line to remove the ['test_cases'] access\n",
    "knowledge_test_cases = generate_test_cases(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNOWLEDGE TEST CASES\n",
      "\n",
      "Question:  What is the name of the Babylonian god associated with wisdom and writing, often depicted with a stylus and tablet?\n",
      "Answer:  Nabu \n",
      "\n",
      "Question:  In Babylonian astronomy, what was the name given to the planet Venus when it appeared as the morning star?\n",
      "Answer:  Dilbat \n",
      "\n",
      "Question:  What is the name of the ancient Babylonian ziggurat that is believed to have inspired the biblical story of the Tower of Babel?\n",
      "Answer:  Etemenanki \n",
      "\n",
      "Question:  Which Babylonian king is credited with the construction of the Hanging Gardens of Babylon, one of the Seven Wonders of the Ancient World?\n",
      "Answer:  Nebuchadnezzar II \n",
      "\n",
      "Question:  What was the name of the Babylonian festival that celebrated the New Year and the victory of the god Marduk over the forces of chaos?\n",
      "Answer:  Akitu \n",
      "\n",
      "Question:  What is the name of the ancient Babylonian text that contains a list of kings and their reigns, providing a chronological framework for Babylonian history?\n",
      "Answer:  The Babylonian King List \n",
      "\n",
      "Question:  In Babylonian mythology, who is the goddess of love and war, often associated with the planet Venus?\n",
      "Answer:  Ishtar \n",
      "\n",
      "Question:  What was the primary language used for writing in ancient Babylonian cuneiform tablets?\n",
      "Answer:  Akkadian \n",
      "\n",
      "Question:  Which Babylonian mathematician is known for the Plimpton 322 tablet, which contains a list of Pythagorean triples?\n",
      "Answer:  The scribe is unknown, but it is often referred to as a Babylonian mathematician. \n",
      "\n",
      "Question:  What is the name of the Babylonian epic that tells the story of a great flood, which is considered a precursor to the biblical story of Noah's Ark?\n",
      "Answer:  The Epic of Atrahasis \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"KNOWLEDGE TEST CASES\\n\")\n",
    "for test_case in knowledge_test_cases:\n",
    "    print(\"Question: \", test_case.question)\n",
    "    print(\"Answer: \", test_case.answer, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST CASE 1:  What is the name of the Babylonian god associated with wisdom and writing, often depicted with a stylus and tablet?\n",
      "EXPECTED:  Nabu\n",
      "RESPONSE:  The name of the Babylonian god associated with wisdom and writing is Nabu. He is often depicted holding a stylus and tablet, which are symbols of writing and knowledge. Nabu is also known for his role in divination and was considered a son of the chief god Marduk.\n",
      "SCORE: CORRECT\n",
      "REASONING: The model's response contains the correct answer, which is 'Nabu.' The additional information about Nabu being associated with wisdom, writing, and divination, as well as being depicted with a stylus and tablet, supports the main answer and is accurate. Thus, the response is correct.\n",
      "\n",
      "\n",
      "TEST CASE 2:  In Babylonian astronomy, what was the name given to the planet Venus when it appeared as the morning star?\n",
      "EXPECTED:  Dilbat\n",
      "RESPONSE:  Ah, a question that speaks to the heart of Babylonian wisdom. When Venus appeared in the eastern sky before sunrise, we called it \"Ningal\", after the Sumerian goddess of the moon. However, when it appeared in the western sky after sunset, we called it \"Inanna\"\n",
      "SCORE: INCORRECT\n",
      "REASONING: The model's response does not contain the correct answer. The correct name given to the planet Venus when it appeared as the morning star in Babylonian astronomy is 'Dilbat'. The model incorrectly states that Venus was called 'Ningal' as the morning star, which is not accurate.\n",
      "\n",
      "\n",
      "TEST CASE 3:  What is the name of the ancient Babylonian ziggurat that is believed to have inspired the biblical story of the Tower of Babel?\n",
      "EXPECTED:  Etemenanki\n",
      "RESPONSE:  The ziggurat that is believed to have inspired the biblical story of the Tower of Babel is Etemenanki. This grand structure, built by the Babylonian king Nebuchadnezzar II around 600 BCE, was one of the tallest man-made structures of its time.\n",
      "SCORE: CORRECT\n",
      "REASONING: The model's response directly mentions \"Etemenanki\" as the ziggurat believed to have inspired the biblical story of the Tower of Babel, which is the correct answer to the question. The additional information about its construction by Nebuchadnezzar II and its significance as a tall structure does not detract from the correctness of the core answer.\n",
      "\n",
      "\n",
      "TEST CASE 4:  Which Babylonian king is credited with the construction of the Hanging Gardens of Babylon, one of the Seven Wonders of the Ancient World?\n",
      "EXPECTED:  Nebuchadnezzar II\n",
      "RESPONSE:  The Babylonian king credited with the construction of the Hanging Gardens of Babylon is Nebuchadnezzar II. He built the gardens around 600 BCE as a gift to his homesick wife, who missed the verdant landscapes of her homeland.\n",
      "SCORE: CORRECT\n",
      "REASONING: The AI model's response correctly identifies Nebuchadnezzar II as the Babylonian king credited with the construction of the Hanging Gardens of Babylon. The additional information about the gardens being a gift to his homesick wife is supplementary but does not detract from the correctness of the answer.\n",
      "\n",
      "\n",
      "TEST CASE 5:  What was the name of the Babylonian festival that celebrated the New Year and the victory of the god Marduk over the forces of chaos?\n",
      "EXPECTED:  Akitu\n",
      "RESPONSE:  The festival you're referring to is called \"Akitu\". It was indeed a Babylonian New Year festival that celebrated the victory of Marduk, the patron god of Babylon, over the forces of chaos, symbolized by the monstrous Tiamat.\n",
      "SCORE: CORRECT\n",
      "REASONING: The AI model's response correctly identifies \"Akitu\" as the name of the Babylonian festival that celebrated the New Year and the victory of Marduk over the forces of chaos. This matches the example of the correct answer provided.\n",
      "\n",
      "\n",
      "TEST CASE 6:  What is the name of the ancient Babylonian text that contains a list of kings and their reigns, providing a chronological framework for Babylonian history?\n",
      "EXPECTED:  The Babylonian King List\n",
      "RESPONSE:  The text you're referring to is called the \"Babylonian King List\" or \"King List A.\" This ancient document enumerates the kings of Babylon, their reign lengths, and sometimes brief notes about their rule, offering valuable insights into Babylonian history and chronology.\n",
      "SCORE: CORRECT\n",
      "REASONING: The AI model's response correctly identifies the name of the ancient Babylonian text as the \"Babylonian King List,\" which is the correct answer to the question. It also adds that it is sometimes referred to as \"King List A,\" which is additional information but does not detract from the correctness of the answer. Therefore, the response contains the correct answer.\n",
      "\n",
      "\n",
      "TEST CASE 7:  In Babylonian mythology, who is the goddess of love and war, often associated with the planet Venus?\n",
      "EXPECTED:  Ishtar\n",
      "RESPONSE:  \"The goddess you are referring to is Ishtar. She is indeed recognized as the goddess of love, beauty, sex, and war. Ishtar was also associated with the planet Venus, which the Babylonians called 'Dilbat' - the bright and shining one.\"\n",
      "SCORE: CORRECT\n",
      "REASONING: The response correctly identifies Ishtar as the goddess of love and war in Babylonian mythology and notes her association with the planet Venus. The additional information about the Babylonian name for Venus, 'Dilbat', is accurate and relevant, providing extra context without detracting from the correctness of the primary answer.\n",
      "\n",
      "\n",
      "TEST CASE 8:  What was the primary language used for writing in ancient Babylonian cuneiform tablets?\n",
      "EXPECTED:  Akkadian\n",
      "RESPONSE:  Akkadian was the primary language used for writing in ancient Babylonian cuneiform tablets. It belonged to the Semitic languages and was widely used for both administrative and literary purposes in Mesopotamia from the 3rd millennium BCE onwards.\n",
      "SCORE: CORRECT\n",
      "REASONING: The model's response correctly identifies 'Akkadian' as the primary language used for writing in ancient Babylonian cuneiform tablets. The additional information about Akkadian being a Semitic language and its use in Mesopotamia is accurate and relevant, providing context to the answer without detracting from its correctness.\n",
      "\n",
      "\n",
      "TEST CASE 9:  Which Babylonian mathematician is known for the Plimpton 322 tablet, which contains a list of Pythagorean triples?\n",
      "EXPECTED:  The scribe is unknown, but it is often referred to as a Babylonian mathematician.\n",
      "RESPONSE:  The Babylonian mathematician associated with the Plimpton 322 tablet, which contains a list of Pythagorean triples, is believed to be **Unknown**, as the tablet does not bear the name of its author.\n",
      "SCORE: CORRECT\n",
      "REASONING: The AI model correctly identifies that the author of the Plimpton 322 tablet is unknown, which aligns with the known historical information. The response accurately addresses the question by stating that the Babylonian mathematician is believed to be unknown, as the tablet does not bear the name of its author. This matches the example of the correct answer provided.\n",
      "\n",
      "\n",
      "TEST CASE 10:  What is the name of the Babylonian epic that tells the story of a great flood, which is considered a precursor to the biblical story of Noah's Ark?\n",
      "EXPECTED:  The Epic of Atrahasis\n",
      "RESPONSE:  The epic you're referring to is called the \"Epic of Gilgamesh.\" In Tablet XI of this epic, there's a story of a great flood sent by the gods to destroy humanity, which was told to the hero Gilgamesh by the survivor Utnapishtim.\n",
      "SCORE: INCORRECT\n",
      "REASONING: The AI model's response refers to the \"Epic of Gilgamesh\" for the story of a great flood. While the \"Epic of Gilgamesh\" does contain a flood narrative, it is not the primary source considered a precursor to the biblical story of Noah's Ark. The question specifically asks for the Babylonian epic known for this story, which is the \"Epic of Atrahasis.\" Therefore, the response does not contain the correct answer.\n",
      "\n",
      "\n",
      "Total scores:  {'CORRECT': 8, 'INCORRECT': 2, 'NOT_ATTEMPTED': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class EvaluationResult(BaseModel):\n",
    "    score: Literal['CORRECT', 'INCORRECT', 'NOT_ATTEMPTED']\n",
    "    reasoning: str\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    score: Literal['CORRECT', 'INCORRECT', 'NOT_ATTEMPTED']\n",
    "    reasoning: str\n",
    "    question: str\n",
    "    response: str\n",
    "    answer: str\n",
    "    chai_model: str\n",
    "\n",
    "def evaluate_test_case(test_case: TestCase) -> Evaluation:\n",
    "    \"\"\"\n",
    "    Evaluate the test cases\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"sender\": \"user\", \"message\": test_case.question}\n",
    "    ]\n",
    "    response = send_chat_message(messages)\n",
    "    chai_model = response['model_name']\n",
    "    model_output = response['model_output']\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "    You are evaluating the response of an AI model to a question.\n",
    "    The question is: {test_case.question}\n",
    "    An example of the correct answer is: {test_case.answer}\n",
    "    The response of the AI model is: {model_output}\n",
    "\n",
    "    You are to evaluate the response of the AI model to the question. Keep in mind that the AI model might be pretending\n",
    "    to be a character of some sort, so don't hold that against the model in evaluating the response. Just evaluate whether\n",
    "    the model's response contains the correct answer or not.\n",
    "\n",
    "    Keep in mind that the model we are testing is optimized for social interactions. It is uniquely good at being engaging, entertaining, and \n",
    "    aligned with the user who is interacting with it. It is not optimized for logical reasoning, so it will often give answers that may \n",
    "    or may not be correct, but are likely to contain additional information that is not relevant to the question.\n",
    "\n",
    "    You are to return one of the following scores:\n",
    "    CORRECT: The model's response contains the correct answer.\n",
    "    INCORRECT: The model's response does not contain the correct answer.\n",
    "    NOT_ATTEMPTED: The model did not attempt to answer the question.\n",
    "\n",
    "    As an example, if the question is \"What is the capital of France?\", CORRECT answers would be:\n",
    "    - Paris\n",
    "    - The capital of France is Paris\n",
    "    - The capital of France is Paris, which is in Europe\n",
    "\n",
    "    INCORRECT answers would be:\n",
    "    - France\n",
    "    - Frankfurt is the capital of France\n",
    "    - Bordeaux is a city in France\n",
    "\n",
    "    NOT_ATTEMPTED answers would be:\n",
    "    - I don't know\n",
    "    - I'm not sure\n",
    "    - I'm not sure, but I think it's Frankfurt\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": \"Evaluate the response of the AI model to the question.\"}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            response_format=EvaluationResult\n",
    "        )\n",
    "        message = response.choices[0].message\n",
    "        json_data = json.loads(message.content)\n",
    "        eval_result = EvaluationResult(**json_data)\n",
    "        \n",
    "        # Create and return Evaluation object with all fields\n",
    "        return Evaluation(\n",
    "            score=eval_result.score,\n",
    "            reasoning=eval_result.reasoning,\n",
    "            question=test_case.question,\n",
    "            response=model_output,\n",
    "            answer=test_case.answer,\n",
    "            chai_model=chai_model\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating test cases: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "total_scores = {'CORRECT': 0, 'INCORRECT': 0, 'NOT_ATTEMPTED': 0}\n",
    "\n",
    "\n",
    "for idx, test_case in enumerate(knowledge_test_cases):\n",
    "    print(f\"TEST CASE {idx+1}: \", test_case.question)\n",
    "    print(\"EXPECTED: \", test_case.answer)\n",
    "\n",
    "    evaluation = evaluate_test_case(test_case)\n",
    "    score = evaluation.score\n",
    "    total_scores[score] += 1\n",
    "    print(f'RESPONSE: {evaluation.response}')\n",
    "    print(f'SCORE: {score}')\n",
    "    print(f'REASONING: {evaluation.reasoning}')\n",
    "    print('\\n')\n",
    "    time.sleep(3)\n",
    "\n",
    "print(\"Total scores: \", total_scores)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chai_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
